# Awe is characterized as an ambivalent affect in the human behavior and cortex  
**Jinwoo Lee**, Danny Dongyeop Han, Seung-Yeop Oh, & **Jiook Cha**.   
Seoul National University   
- **First Author:** Jinwoo Lee, M.S. (adem1997@snu.ac.kr | [jinwoo-lee.com](https://jinwoo-lee.com))   
- **Corresponding Author:** Jiook Cha, Ph.D. (connectome@snu.ac.kr | [www.connectomelab.com](https://www.connectomelab.com))

## Abstract
Awe is a complex emotion that encompasses conflicting affective feelings inherent to its key appraisals, but it has been studied as either a positive or a negative emotion, which has made its ambivalent nature underexplored. To address whether and how awe's ambivalent affect is represented both behaviorally and neurologically, we conducted a study using virtual reality (VR) and electroencephalography (*N* = 43). Behaviorally, the subjective ratings of awe intensity for VR clips were best predicted by the duration and intensity of ambivalent feelings, not by single valence-related metrics. In the electrophysiological analysis, we identified a latent neural-feeling space for each participant that shared valence representations across individuals and stimuli, using deep representational learning and decoding analyses. Within these spaces, ambivalent feelings were represented as spatially distinct from positive and negative ones with large individual differences in their separation. Notably, this variability significantly predicted subjective awe ratings. Lastly, hidden Markov modeling revealed that the multiple band powers, particularly in the frontoparietal channels were significantly associated with differentiation of valent states during awe-inducing VR watching. Our findings suggest that awe is better characterized as ambivalent affect that integrates opposing feelings than simply positive and negative ones. This work provides a nuanced framework for understanding the complexity of human emotions with implications for affective neuroscience.
 

![AweVR_video_abstract](https://github.com/user-attachments/assets/c122bc4e-7af7-497e-ab87-9682a529ebab)

## Data Availiability  
Before getting started, please make sure you have downloaded all the dataset required to run the code from our Open Science Framework repository.


## Guided Tour for a Code Structure  
In general, we utilized two programming languages, Python and R, for the following goal, respectively:
- **Python**: Feature extraction with CEBRA, PCA, and HMM.
- **R**: Statistical analysis with extracted features and visulization.   
   
In the following section, we will explain how each code was utilized in different stages of analysis.   

### Part I. Behavioral Analysis ###
- **(01) `behavioral_analysis_n43.R`**: This code performs statistical analyses and predictive modeling based on self-report data. It covers the following sections and contents:
  
  - (Results) section *'Awe is associated with longer and stronger ambivalent feelings, but not with univalent ones'*.
  - (Results) section *'Ambivalent feelings predict the awe ratings more precisely than univalent metrics'*.
  - (Contents) Fig 2 and 3, and Supplementary Table 1 and 3.


### Part II. Valence Decoding Analysis ###
- **(02) `cebra_decoding_n06.py`**: Using CEBRA, this code learns a latent neural-feeling space based on each participant–clip pair's EEG and valence data, and evaluates the performance of 'across-participants' and 'across-clips' decoding tasks under the random, not aligned, and aligned conditions. It generates two outputs - `across_clip_decoding_CEBRA.csv` and `across_sub_decoding_CEBRA.csv`, which serve as inputs for **Code (05)**.
  
  > **NOTE** For reproducibility, we recommend running **Code (05)** directly using the pre-generated .csv files to perform the statistical analyses. This is because CEBRA does not support seed fixing, and retraining the model from scratch may lead to slight variations in the actual statistics, even if the overall patterns remain consistent.

- **(03) `pca_decoding_n06.py`**: This code operates in the same manner as **Code (02)**, but uses PCA instead of CEBRA. It generates two outputs - `across_clip_decoding_PCA.csv` and `across_sub_decoding_PCA.csv`, which serve as inputs for **Code (05)**.

- **(04) `faa_decoding_n06.py`**: This code operates in the same manner as **Code (02)**, but uses FAA (frontal alpha asymmetry) instead of CEBRA. It generates two outputs - `across_clip_decoding_FAA.csv` and `across_sub_decoding_FAA.csv`, which serve as inputs for **Code (05)**.

- **(05) `decoding_stat_analysis_n06.R`**: This code uses the six .csv files generated by **Codes (02), (03), and (04)** to explore the optimal dimensionality of the latent spaces derived from CEBRA and PCA, and examines statistical differences in decoding performance accordingly. It covers the results reported in the following sections and contents:

  - (Results) section *'Aligned latent neural-feeling spaces share geometric structures of valence representation across individuals and stimuli'*.
  - (Contents) Fig 4, Supplementary Fig 2, Supplementary Table 4.
 

### Part III. Latent Space Analysis ###
- **(06) `cebra_latent_space_construction_n27.py`**: Using CEBRA, this code trains one true model and 1,000 pseudo models per participant–clip pair for permutation testing. Based on these models, it computes the silhouette coefficient of valence clusters and cortical distinctiveness metrics in the latent space. It generates each participant’s `{sub}_total_summary.csv` file, which serves as input for **Code (07)**.

  > **NOTE** For reproducibility, we recommend running **Code (07)** directly using the pre-generated .csv files to perform the statistical analyses. This is because CEBRA does not support seed fixing, and retraining the model from scratch may lead to slight variations in the actual statistics, even if the overall patterns remain consistent.

- **(07) `latent_stat_analysis_n27.R`**: This code takes the participant-level .csv files generated by **Code (06)** as input and covers the results reported in the following sections and contents of the manuscript:
  
  - (Results) section *'The more distinctively ambivalent feelings are representated in the cortices, the more saliently individuals report awe'*.
  - (Contents) Fig 5, Supplementary Fig 3.


### Part IV. Hidden Markov Modeling (HMM) analysis ###
- **(08) `hmm_channel_n06.py`**: This code uses a HMM to compute the match rate of each channel, along with its null distribution via permutation testing. It then generates the `hmm_match_rate_channel.csv` file, which serves as input for **Code (10)**.

- **(09) `hmm_freqband_n06.py`**: This code operates in the same manner as **Code (08)**, but instead produces the `hmm_match_rate_freqband.csv` file, which contains results for each frequency band.

- **(10) `hmm_match_rate_analysis_n06.R`**: "This script takes as input the .csv files generated by **Code (08) and (09)**, and identifies EEG features that exhibit qualitatively distinct patterns across different valence states. It corresponds to the results reported in the following sections and contents of the manuscript:
  
  - (Results) section *'The multiple band frequency powers and frontoparietal channels mainly distinguish different affective states during awe'*.
  - (Contents) Fig 6, Supplementary Table 5. 

### Part V. Sensory-Affect Analysis ###
- **(11) `sensory_affect_analysis_n43.R`**: This script visualizes changes in the audiovisual features of each VR video and tests their association with participants’ affective response patterns. It supports the results reported in the following sections and contents of the manuscript:

  - (Contents) Supplementary Fig 1b, Supplementary Table 2.

## Citation
*(TBD)*
